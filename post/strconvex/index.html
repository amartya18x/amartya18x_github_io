<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.20.7" />
  <meta name="author" content="Amartya Sanyal">
  <meta name="description" content="D.Phil Student in Computer Science">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="https://amartya18x.github.io/index.xml" type="application/rss+xml" title="Amartya">
  <link rel="feed" href="https://amartya18x.github.io/index.xml" type="application/rss+xml" title="Amartya">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://amartya18x.github.io/post/strconvex/">

  

  <title>Strong Convexity and Strong Smoothness | Amartya</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Amartya</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/post/about_me/">
            
            <span>About Me</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  

  <div class="article-container">
    <h1 itemprop="name">Strong Convexity and Strong Smoothness</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="2016-09-15 23:49:40 &#43;0530 IST" itemprop="datePublished">
      Thu, Sep 15, 2016
    </time>
  </span>

  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2famartya18x.github.io%2fpost%2fstrconvex%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Strong%20Convexity%20and%20Strong%20Smoothness&amp;url=https%3a%2f%2famartya18x.github.io%2fpost%2fstrconvex%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2famartya18x.github.io%2fpost%2fstrconvex%2f&amp;title=Strong%20Convexity%20and%20Strong%20Smoothness"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2famartya18x.github.io%2fpost%2fstrconvex%2f&amp;title=Strong%20Convexity%20and%20Strong%20Smoothness"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Strong%20Convexity%20and%20Strong%20Smoothness&amp;body=https%3a%2f%2famartya18x.github.io%2fpost%2fstrconvex%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<p>I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular <strong>gradient descent</strong> works on them. So, before going right into the details let&rsquo;s have a quick chat about convexity in general and we do have a few ways of going about it.</p>

<p>I will go about with talking about two definitions of convex functions, the first one being general more than the second. This is not to say that there aren&rsquo;t more general or equivalent definitions, which there are, but that I like these better and this is probably what you will use more in your life if you have to use one at all.</p>

<h2 id="convex-function">Convex Function</h2>

<p>$$\frac{f(y)+f(x)}{2} \ge f(\frac{x+y}{2}) $$
Note that this does not require the function to be differentiable. But, with differetiable functions, one can actually get an easier and more productive definition.</p>

<h3 id="convex-differetiable-functions">Convex differetiable functions</h3>

<p>$$ f(y) \ge f(x) + \langle \nabla f(x), y - x \rangle $$</p>

<h3 id="subgradients">Subgradients</h3>

<p>The interesting thing about this is that this can tolerate functions, which are not differntiable in a finite number of points. We need to define <strong>subgradients</strong> for that though.</p>

<p>$$ g(x) = \{ g(x) | \langle g(x), x_0 - x\rangle \forall y \} $$</p>

<p>And hence let&rsquo;s do the intuitive thing of replacing  $\nabla f(x)$ with $g(x)$. So, now we have</p>

<p>$$ f(y) \ge f(x) + \langle  g(x), y - x \rangle $$</p>

<h2 id="strongly-convex-function">Strongly Convex function</h2>

<p>Now that we know of convex functions mathematically, it is intuitively a function such that if we draw a tangent plane(line in case of one variable), the function <strong>lies above the tangent plane</strong> at all points. Mathematically, that would be</p>

<p>$$ f(y) \ge f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2 $$ and <strong>$\alpha$</strong> is known as the strong convexity parameter. One can also, for doubly differntiable functions, say $ \nabla^2 f(x) \succeq \alpha I  $, where $\nabla^2 f(x)$ is the hessian matrix and $ I $ is the identity matrix.</p>

<h2 id="strongly-smooth-function">Strongly Smooth function</h2>

<p>A strongly smooth function is just the opposite of the strongly convex function i.e. a function which <strong>lies below the tangent plane</strong> at all points. Mathematically, that would be</p>

<p>$$ f(y) \le f(x) + \langle g(x), y - x \rangle + \frac{\beta}{2} \| y - x\|^2 $$ and <strong>$\beta$</strong> is known as the strong smoothness parameter. One can also, for doubly differntiable functions, say $ \beta I \succeq \nabla^2 f(x)  $.</p>

<h1 id="gradient-descent-for-strongly-convex-functions">Gradient descent for strongly convex functions</h1>

<p>We know for a strongly convex function $f(x)$, $$ f(y) \ge f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2 $$ A very nice property of these functions is that, we can actually bound $\|f( x^{*}) - f(x)\|$ where $(x^{*})$ is the optimal point , with the norm of the gradient ($\|\nabla f(x)\|$).
Let,
$$
z = \underset{y}{argmin }\hspace{5pt}  \{ f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2 \}
$$</p>

<p>The definition of strong convexity is applicable for all y.</p>

<p>\begin{align*}
z &amp;= x - \frac{\nabla f(x)}{\alpha} \\<br />
f(y) &amp;\ge f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2  \\<br />
&amp;\ge f(x) + \langle g(x), \frac{\nabla f(x)}{\alpha}  \rangle + \frac{\alpha}{2} \| \frac{\nabla f(x)}{\alpha}  \|^2 \\<br />
&amp;\ge f(x) - \frac{1}{2\alpha} | \nabla f(x)|^2 \\<br />
\therefore f(x^{*}) &amp;\ge f(x) - \frac{1}{2\alpha} \| \nabla f(x)\|^2
\end{align*}</p>

<p>This interesting bound also gives us a good convergence criterion. Set $f(x) - f(x^{*}) = \epsilon$ i.e. the desired error. We only need to do the gradient descent until the gradient has reached $2\alpha \epsilon$ from the previous inequality.
$$
    2\alpha \epsilon \le \frac{1}{2\alpha} \| \nabla f(x)\|^2
$$</p>

<p>Like the function value above, we can also get a bound on the distance of the current point from the optimal point in terms of gradient.</p>

<p>\begin{align*}
    f(x^{*})  &amp;\ge f(x) + \langle g(x), x^{*} - x \rangle + \frac{\alpha}{2} \| x^{*} - x\|^2  \\<br />
     &amp;\ge f(x) - \underbrace{( \{ \| g(x)\|\| x^{*} - x \| - \frac{\alpha}{2} \| x^{*} - x\|^2 \}) }_a &amp;&amp; \text{(By Cauchy&rsquo;s inequality)}\\<br />
\end{align*}
As we know that $f(x^{*})$ is optimal $ f(x^{*}) \le f(x)$, a \ge 0 must be true.</p>

<p>$$ \| g(x)\|\| x^{*} - x \| \ge \frac{\alpha}{2} \| x^{*} - x\|^2  $$
Or,
$$\frac{2}{\alpha} \| g(x)\| \ge   \| x^{*} - x\|  $$</p>

<h2 id="analyzing-the-gradient-descent">Analyzing the gradient descent</h2>

<p>Define,
$$
    \Phi_t = f(x^t) - f(x^{*})
$$<br />
- the lyapunov function. Decrease in the value of the lyapunov function means that the function is nearing its optima. Let&rsquo;s also define another distance.
$$
    D_t = \| x^{*} - x_t \|_2
$$
\begin{align*}
    \label{eq:phi}
     f(x^{*})  &amp;\ge f(x_t) + \langle \nabla f(x_t), x^{*} - x_t \rangle + \frac{\alpha}{2} \| x^{*} - x_t\|^2  \\<br />
     f(x^{*}) -  f(x_t)  &amp;\ge \hspace{2pt} \langle\nabla f(x_t), x^{*} - x_t \rangle + \frac{\alpha}{2} \| x^{*} - x_t\|^2 \\<br />
\end{align*}</p>

<p>Hence, we have the following inequality with the lyapunov function.
\begin{equation}
     \Phi_t \le -\langle g(x_t), x^{*} - x_t \rangle - \frac{\alpha}{2} \| x^{*} - x_t\|^2
\end{equation}
Lets work with $D_t$ and then try to relate it with $\Phi_t$.
\begin{align*}
    D^2_{t+1} &amp;= \|x^{t+1} - x^{*} \|_2^2 \\<br />
    &amp;= \| x^t - \eta_t g_t -   x^{*}\|_2^2 \\<br />
    &amp; = \|x^t - x^{*} \|_2^2 + \eta_t^2 \|g_t(x_t)\|^2  - 2 \eta_t   \langle x^t - x^{*}, g_t\rangle \\<br />
    D^2_{t+1} - D^2_{t} &amp;= \eta_t^2 \|g_t(x_t)\|^2  - 2 \eta_t   \langle x^t - x^{*}, g_t\rangle \\<br />
    \langle x^t - x^{*}, g_t\rangle &amp;= \frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2}
\end{align*}
Let&rsquo;s plugin in this onto the previous definition of the lyapunov function.
\begin{align*}
    \Phi_t &amp;\le -\langle g(x_t), x^{*} - x_t \rangle - \frac{\alpha}{2} \| x^{*} - x_t\|^2 \\<br />
    &amp;\le \frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} \| x^{*} - x_t\|^2 \\<br />
    &amp;\le  \frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} D^2_t
\end{align*}
It is difficult to show that $\Phi_t$ is going to zero. It is considerably easier to use the sum for that. We will see a trick with jensen&rsquo;s inequality.
\begin{align*}
    \sum_{t=0}^T \Phi_t &amp;\le \sum_{t=0}^T (\frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} D^2_t) \\<br />
    &amp;\le  \sum_{t=0}^T (\frac{D^2_t}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} D^2_t) -  \sum_{t=1} \frac{D^2_{t}}{2\eta_{t-1}} &amp;&amp; \text{(A bit of change of variable)} \\<br />
    &amp;\le D_0^2(\frac{1}{2\eta_0} - \frac{\alpha}{2}) +  \sum_{t=1}^T D^2_t(\frac{1}{2\eta_t} - \frac{\alpha}{2} + \frac{1}{2\eta_{t-1}}) + \sum_{t=0}^T\frac{\eta_t G^2}{2} &amp;&amp;\text{(Assuming bounded gradients again)}
\end{align*}</p>

<p>Now, the first term is a constant and the third term is a sum of constants weighed by a parameter that we are fixing. So, the major problem is with the second term. Why note set it to zero ? We can do that by setting <code>$\eta\_t = \frac{1}{\alpha t}$</code>
\begin{align*}
    \sum_{t=0}^T \Phi_t &amp;\le D_0^2(\underbrace{\frac{1}{2\eta_0} - \frac{\alpha}{2}}_{\le 0}) +  \sum_{t=1}^T D^2_t( \underbrace{\frac{1}{2\eta_t} - \frac{\alpha}{2} + \frac{1}{2\eta_{t-1}}}_0) + \sum_{t=0}^T\frac{ G^2}{2\alpha t} &amp;&amp;\text{(Assuming bounded gradients again)} \\<br />
    \frac{1}{T}\sum_{t=0}^T {\Phi_t} &amp;\le \frac{G^2 log(T)}{2T} \\<br />
    \sum_{t=0}^T  \frac{f(x^t) - f(x^{*})}{T} &amp;\le \frac{G^2 log(T)}{2T}  \\<br />
    \end{align*}
Applying Jensen&rsquo;s inequality because $\Phi_t$ is convex
         $$f(\frac{\sum_{t=0}^T x^t}{T}) \le \sum_{t=0}^T  \frac{f(x^t) }{T} - f(x^{*}) \le \frac{G^2 log(T)}{2T} $$
To remove the $log(T)$, try doing a weighed sum of the $\Phi_t$ with $t\Phi_(t)$ and then don&rsquo;t forget to divide by $\sum_{t=1}^{T}t$. It will work out smooth</p>

<h1 id="gradient-descent-for-strongly-smooth-functions">Gradient descent for strongly smooth functions</h1>

<p>We will work with similar <strong>Lyapunov function</strong> <code>$\Phi\_t$</code> and <code>$D\_t$</code></p>

<p>As $f(x)$ is strong smooth,
$$ f(x^{t+1}) \le f(x^t) + \langle \nabla f(x^t), x^{t+1} - x^t \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 $$
and as $f(x)$ is convex
$$ f(x^{*}) \ge f(x) + \langle \nabla f(x), x^{*} - x \rangle  $$
By rearranging terms we get,
$$  f(x) \le f(x^{*})  + \langle \nabla f(x), x - x^{*}  \rangle  $$
Plugging in this inequality into the strong smoothness inequality and replacing the $\Phi_{t+1}$ and $D_t$ in the correct places gives us the following</p>

<p>\begin{align*}
\Phi_{t+1} &amp;\le   \langle \nabla f(x), x^t - x^{*}\rangle + \langle \nabla f(x^t), x^{t+1} - x^t \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br />
&amp;\le  \langle \nabla f(x), x^t - x^{*} +  x^{t+1} - x^t \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br />
&amp;\le  \langle \nabla f(x),  x^{t+1} - x^{*} \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br />
\end{align*}</p>

<p>Now, we know the following <code>$x^{t+1} = x^t - \eta\_t \nabla f(x^t)$</code> which gives us <code>$\nabla f(x^t) = \frac{x^t - x^{t+1}}{\eta\_t} $</code></p>

<p>\begin{align*}
\Phi_{t+1} &amp;\le  \langle \nabla f(x),  x^{t+1} - x^{*} \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br />
    &amp;\le  \frac{1}{\eta_t}\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br />
\end{align*}
Now, we need to evaluate the term <code>$\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle $</code>
\begin{align*}
\|x^t - x^{*}\|^2 &amp;= \|x^t - x^{t+1} + x^{t+1} - x^{*}\| \\<br />
    &amp;=\|x^t - x^{t+1}\|^2 + \|x^{t+1} - x^{*}\| + 2\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle \\<br />
\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle &amp;= \frac{1}{2}( D_{t}^2 - D_{t+1}^2 -  \|x^{t+1} - x^{*}\| )
\end{align*}
Plugging this back into the inequality we have above, we get the following</p>

<p>\begin{align*}
\Phi_{t+1}     &amp;\le  \frac{1}{2\eta_t} (D_{t}^2 - D_{t+1}^2 -  \frac{1}{2}\|x^{t+1} - x^{*}\| )+ \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br />
&amp;\le \frac{1}{2\eta_t} (D_{t}^2 - D_{t+1}^2) - \| x^{t+1} - x^t \|^2 (\frac{1}{2\eta_t} - \frac{\beta}{2})
\end{align*}
Set  $\eta_t = \frac{c}{\beta} $  where, $c\in [0,1]$ . We get($0 \ge k\le 1) $
$$ (\frac{1}{2\eta_t} - \frac{\beta}{2}) = \frac{\beta}{2} (\frac{1}{k} - 1)\ge 0  $$</p>

<p>\begin{align*}
\sum_{t=0}^T \Phi_{t+1} &amp;\le \sum_{t=0}^T \frac{\beta}{2c} (D_{t}^2 - D_{t+1}^2)\\<br />
\frac{1}{T} \sum_{t=0}^T \Phi_{t+1}  &amp;\le \frac{\beta}{2cT} (D_{0}^2 - D_{T+1}^2)
\end{align*}
Applying Jensen&rsquo;s inequality because $\Phi_t$ is convex
         $$f(\frac{\sum_{t=0}^T x^t}{T}) \le \sum_{t=0}^T  \frac{f(x^t) }{T} - f(x^{*}) \le \frac{\beta}{2cT} (D_{0}^2 ) $$</p>

<p>For both the strongly convex and the strong smooth functions, we have seen the average selector. Is it possible that some other selector can give us better bounds ? Selector refers to the particular way of choosing the $x$, which we want the function to return.</p>

    </div>
  </div>

</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://amartya18x.github.io/post/poems/"><span
      aria-hidden="true">&larr;</span> The Cemetery of the mind!</a></li>
    

    
    <li class="next"><a href="https://amartya18x.github.io/post/nn_nonconvex/">NonConvexity of neural networks <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2016 Your Name &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    <script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    

  </body>
</html>

